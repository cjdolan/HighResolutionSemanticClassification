{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjdolan/HighResolutionSemanticClassification/blob/main/ATML_Build_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook for automatically building a satellite imagery dataset for semantic classification and object detection.\n",
        "\n",
        "Author: Connor Dolan"
      ],
      "metadata": {
        "id": "ySXJJtj7C7JN"
      },
      "id": "ySXJJtj7C7JN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a1fdd4f"
      },
      "outputs": [],
      "source": [
        "!pip install xmltodict\n",
        "!pip install geopy\n",
        "!pip install d2l\n",
        "!pip install wget\n",
        "!pip install utm\n",
        "!pip install rasterio\n",
        "!pip install swifter"
      ],
      "id": "1a1fdd4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u_6j9ERG00s"
      },
      "outputs": [],
      "source": [
        "USERNAME = \"cjdolan\"\n",
        "PASSWORD = \"1998B@seb@!!29\""
      ],
      "id": "9u_6j9ERG00s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d8f0224"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import json\n",
        "import xml\n",
        "import xml.etree.ElementTree as ET\n",
        "import xmltodict\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urljoin\n",
        "from shapely.geometry import Polygon\n",
        "import numpy as np\n",
        "import wget\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import swifter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import auth\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "auth.authenticate_user()\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "!mkdir bucket_data\n",
        "!gcsfuse --implicit-dirs atml_bucket bucket_data\n",
        "headers = {}\n",
        "payload = {}"
      ],
      "id": "1d8f0224"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3eUcHCT6_D"
      },
      "outputs": [],
      "source": [
        "print(len(np.unique(glob('/content/bucket_data/*.tif'))))"
      ],
      "id": "SQ3eUcHCT6_D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3703b1dd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "r = requests.get('https://overpass-api.de/api/interpreter?data=way[name=%22Fulton%20County%20Airport-Brown%20Field%22];(._;%3E;);way(around:40000)[aeroway=%22aerodrome%22];(._;%3E;);out%20center;')\n",
        "xmltodict.parse(r.text)['osm']['way'][0]['center']"
      ],
      "id": "3703b1dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j9y_r85ECQ8"
      },
      "outputs": [],
      "source": [
        "xmltodict.parse(r.text)['osm']"
      ],
      "id": "8j9y_r85ECQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2fd9c98"
      },
      "outputs": [],
      "source": [
        "def get_name(tags):\n",
        "    if isinstance(tags, list):\n",
        "        for t in tags:\n",
        "            if t['@k'] == 'name':\n",
        "                return t['@v']"
      ],
      "id": "d2fd9c98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91OuuTty0E5Y"
      },
      "outputs": [],
      "source": [
        "def get_node(nodes, way_id):\n",
        "  for n in nodes:\n",
        "    if way_id == n['@id']:\n",
        "      return n\n",
        "\n",
        "def get_bb(nodes, ways):\n",
        "  left = right = top = bottom = None\n",
        "  for w in ways:\n",
        "    node = get_node(nodes, w['@ref'])\n",
        "    lat, lon = node['@lat'], node['@lon']\n",
        "    if left == None:\n",
        "      left = right = lon\n",
        "      top = bottom = lat\n",
        "    else:\n",
        "      if lon < left:\n",
        "        left = lon\n",
        "      if lon > right:\n",
        "        right = lon\n",
        "      if lat < bottom:\n",
        "        bottom = lat\n",
        "      if lat > top:\n",
        "        top = lat\n",
        "  return left, right, top, bottom\n",
        "\n",
        "#get_bb(xmltodict.parse(r.text)['osm']['node'], xmltodict.parse(r.text)['osm']['way'][0]['nd'])"
      ],
      "id": "91OuuTty0E5Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhsPK7Gi7tbK"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['Name', 'Lat', 'Long', 'Class', 'Left', 'Right', 'Top', 'Bottom'])"
      ],
      "id": "hhsPK7Gi7tbK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebc9da23"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "def default_value():\n",
        "    return -1\n",
        "\n",
        "\n",
        "\n",
        "def get_addresses_of_poi(classes, tags, num_samples, position):\n",
        "    result_total = []\n",
        "    for i, t in enumerate(tags):\n",
        "        poi_result = []\n",
        "        seen = defaultdict(default_value)\n",
        "        load_bar = tqdm(total=num_samples[i], position=position, leave=True)\n",
        "        stack = []\n",
        "\n",
        "        r = 40000\n",
        "        n = 'Fulton County Airport-Brown Field'.replace(' ', '%20')\n",
        "        k = t[0]\n",
        "        v = t[1]\n",
        "        query = 'https://overpass-api.de/api/interpreter?data=way(33.76,-84.52,33.78,-84.57)[name=%22{name}%22];(._;%3E;);way(around:{radius})[{key}=%22{value}%22];(._;%3E;);out%20center;'.format(key=k, value=v, name=n, radius=r)\n",
        "        result = requests.get(query)\n",
        "        while 'osm' not in xmltodict.parse(result.text).keys():\n",
        "          time.sleep(2)\n",
        "          result = requests.get(query)\n",
        "        nodes = xmltodict.parse(result.text)['osm']['node']\n",
        "        result = xmltodict.parse(result.text)['osm']['way']\n",
        "        left, right, top, bottom = get_bb(nodes, result[0]['nd'])\n",
        "        stack.append([get_name(result[0]['tag']), result[0]['center']['@lat'], result[0]['center']['@lon'], classes[i],\n",
        "                     left, right, top, bottom])\n",
        "        while stack:\n",
        "            cur = stack[0]\n",
        "            del stack[0]\n",
        "            if seen[cur[0]] < 0:\n",
        "                seen[cur[0]] = 1\n",
        "                poi_result.append(cur)\n",
        "                load_bar.update(1)\n",
        "                center_point = (cur[1], cur[2])\n",
        "                if cur[0] != None:\n",
        "                    try:\n",
        "                        n = cur[0].replace(' ', '%20')\n",
        "                        query = 'https://overpass-api.de/api/interpreter?data=way\\\n",
        "                                      [name=%22{name}%22];\\\n",
        "                                      (._;%3E;);way\\\n",
        "                                      (around:{radius})[{key}=%22{value}%22];\\\n",
        "                                      (._;%3E;);out%20center;'.format(key=k, value=v, name=n, radius=r)\n",
        "                        try:\n",
        "                          result = requests.get(query)\n",
        "                          nodes = xmltodict.parse(result.text)['osm']['node']\n",
        "                          result = xmltodict.parse(result.text)['osm']['way']\n",
        "                        except:\n",
        "                          time.sleep(1)\n",
        "                          result = requests.get(query)\n",
        "                          nodes = xmltodict.parse(result.text)['osm']['node']\n",
        "                          result = xmltodict.parse(result.text)['osm']['way']\n",
        "                        for way in result:\n",
        "                            if isinstance(way, dict):\n",
        "                                if seen[get_name(way['tag'])] < 0:\n",
        "                                  left, right, top, bottom = get_bb(nodes, way['nd'])\n",
        "                                  stack.append([get_name(way['tag']), way['center']['@lat'], way['center']['@lon'], classes[i],\n",
        "                                              left, right, top, bottom])\n",
        "                    except:\n",
        "                        pass\n",
        "            load_bar.set_description('POI: %s   Number of samples: %i' % (classes[i], len(poi_result)))\n",
        "            if len(poi_result) > num_samples[i]:\n",
        "                break\n",
        "\n",
        "        result_total += poi_result\n",
        "\n",
        "    return result_total\n",
        "\n",
        "final_results = []\n",
        "classes = ['power_plant', 'stadium','airport']\n",
        "osm_names = [['power','plant'], ['leisure', 'stadium'], ['aeroway', 'aerodrome']]\n",
        "# pool = ThreadPool(10)\n",
        "# for i in range(len(classes)):\n",
        "#   final_results.append(pool.apply_async(get_addresses_of_poi, args=([classes[i]], [osm_names[i]], [1500], i)))\n",
        "# pool.close()\n",
        "# pool.join()\n",
        "# final_results = [r.get() for r in final_results]\n",
        "r = get_addresses_of_poi(classes, osm_names, [1500, 1500, 1500], 0)\n",
        "result = []\n",
        "for c in final_results:\n",
        "  for elem in c:\n",
        "    result.append(elem)\n",
        "new_df = pd.DataFrame(r, columns=['Name', 'Lat', 'Long', 'Class', 'Left', 'Right', 'Top', 'Bottom'])\n",
        "df = pd.concat([df, new_df])"
      ],
      "id": "ebc9da23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkgaA_nLkmn2"
      },
      "outputs": [],
      "source": [
        "df"
      ],
      "id": "kkgaA_nLkmn2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrkL1m7jkAd"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/Temp_DF_Checkpoint.csv')"
      ],
      "id": "FFrkL1m7jkAd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u8ojtRMrZdr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Temp_DF_Checkpoint.csv')"
      ],
      "id": "5u8ojtRMrZdr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48128dc0"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
        "url = API_URL\n",
        "session = requests.Session()\n",
        "\n",
        "def login(username, password):\n",
        "  \"\"\"Get an API key.\n",
        "  Parameters\n",
        "  ----------\n",
        "  username : str\n",
        "      EarthExplorer username.\n",
        "  password : str\n",
        "      EarthExplorer password.\n",
        "  \"\"\"\n",
        "  login_url = urljoin(url, \"login\")\n",
        "  payload = {\"username\": username, \"password\": password}\n",
        "  r = session.post(login_url, json.dumps(payload))\n",
        "  session.headers[\"X-Auth-Token\"] = r.json().get(\"data\")\n",
        "\n",
        "def get_resolution(metadata):\n",
        "  for m in metadata:\n",
        "    if m['fieldName'] == 'Resolution':\n",
        "      return m['value']\n",
        "\n",
        "def scene_search(lat1, lon1, lat2, lon2, datasetName='NAIP', maxResults=5):\n",
        "  search_url = urljoin(url, \"scene-search\")\n",
        "  spatialFilter = {\n",
        "      'filterType': 'mbr',\n",
        "      'lowerLeft': {\n",
        "          'latitude':lat1,\n",
        "          'longitude':lon1\n",
        "      },\n",
        "      'upperRight': {\n",
        "          'latitude':lat2,\n",
        "          'longitude':lon2\n",
        "      }\n",
        "  }\n",
        "  cloudCoverFilter = {'min':0, 'max':5}\n",
        "  sceneFilter = {'spatialFilter':spatialFilter, 'cloudCoverFilter':cloudCoverFilter}\n",
        "  search_request = {'datasetName': datasetName, 'maxResults': maxResults, 'sceneFilter':sceneFilter, 'metadataType':'full'}\n",
        "  r = session.post(search_url, json.dumps(search_request)).json()\n",
        "  for result in r['data']['results']:\n",
        "    res = get_resolution(result['metadata'])\n",
        "    if float(res) == 1.0:\n",
        "      return result['entityId'], result['spatialCoverage']['coordinates'][0]\n",
        "\n",
        "\n",
        "def get_product_id(entityId):\n",
        "  search_url = urljoin(url, \"download-options\")\n",
        "  payload = {'datasetName':'NAIP', 'entityIds':[entityId]}\n",
        "  r = session.post(search_url, json.dumps(payload)).json()\n",
        "  return r['data'][0]['id'], r['data'][0]['displayId']\n",
        "\n",
        "\n",
        "def download_scene(entityId, productId):\n",
        "  dl_req_url = urljoin(url, \"download-request\")\n",
        "  download_request = {\n",
        "      'downloads': [{\n",
        "        'entityId': entityId,\n",
        "        'productId': productId\n",
        "      }]\n",
        "  }\n",
        "  r = session.post(dl_req_url, json.dumps(download_request)).json()\n",
        "  return r\n",
        "\n",
        "def bulk_download_scene(entityId, productId):\n",
        "  dl_req_url = urljoin(url, \"download-request\")\n",
        "  downloads = []\n",
        "  for i, e in enumerate(entityId):\n",
        "    downloads.append({'entityId':e, 'productId':productId[i]})\n",
        "  download_request = {\n",
        "      'downloads': downloads\n",
        "  }\n",
        "  r = session.post(dl_req_url, json.dumps(download_request)).json()\n",
        "  return r\n",
        "\n",
        "def download_retrieve():\n",
        "  dl_req_url = urljoin(url, \"download-retrieve\")\n",
        "  download_request = {\n",
        "  }\n",
        "  r = session.post(dl_req_url, json.dumps(download_request)).json()\n",
        "  return r\n",
        "\n",
        "def remove_download(downloadId):\n",
        "  dl_req_url = urljoin(url, \"download-remove\")\n",
        "  downloads = []\n",
        "  download_request = {\n",
        "      'downloadId': downloadId\n",
        "  }\n",
        "  r = session.post(dl_req_url, json.dumps(download_request)).json()\n",
        "  return r\n",
        "\n",
        "def build_bb(points):\n",
        "  return [(points[1], points[0]), (points[1], points[2]), (points[3], points[0]), (points[3], points[2])]\n",
        "\n",
        "def overlap2(rect1,rect2):\n",
        "    p1 = Polygon([rect1[0], rect1[1],rect1[2],rect1[3]])\n",
        "    p2 = Polygon([rect2[0], rect2[1],rect2[2],rect2[3]])\n",
        "    return(p1.intersects(p2))\n",
        "rect1 = [[-84.6300721, 33.9973916], [-84.5593611, 33.9960083], [-84.5573805, 34.0651027], [-84.6281499, 34.0664888]]\n",
        "rect2 = build_bb([34.0106131, -84.6085182, 34.0181462, -84.5835407])\n",
        "print(overlap2(rect1,rect2))"
      ],
      "id": "48128dc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBVuXWVHz2vu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        " \n",
        "# Data to be written\n",
        "dictionary = {\n",
        "    \"username\": USERNAME,\n",
        "    \"password\": PASSWORD\n",
        "}\n",
        " \n",
        "with open(\"data.json\", \"w\") as outfile:\n",
        "    json.dump(dictionary, outfile)"
      ],
      "id": "UBVuXWVHz2vu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqeDesD8WFVC"
      },
      "outputs": [],
      "source": [
        "read_df = False\n",
        "if read_df:\n",
        "  df = pd.read_csv('/content/drive/MyDrive/GeoData_large.csv')"
      ],
      "id": "mqeDesD8WFVC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l658-2NMtuH2"
      },
      "outputs": [],
      "source": [
        "df"
      ],
      "id": "l658-2NMtuH2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RHSXALsxDVHp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import json\n",
        "with open('data.json', 'r') as openfile:\n",
        "  json_file = json.load(openfile)\n",
        "  username, password = json_file['username'], json_file['password']\n",
        "  login(username, password)\n",
        "  df['entity_id'] = ''\n",
        "  df['product_id'] = ''\n",
        "  for index, row in tqdm(df.iterrows(), total=df.shape[0], leave=True, position=0):\n",
        "    if row.entity_id == '':\n",
        "      try:\n",
        "        entityId, scene_box = scene_search(float(row.Bottom), float(row.Right), float(row.Top), float(row.Left))\n",
        "        product_id, file_name = get_product_id(entityId)\n",
        "        df.at[index, 'entity_id'] = entityId\n",
        "        df.at[index, 'product_id'] = product_id\n",
        "        df.at[index, 'fileName'] = file_name\n",
        "        temp = df[df.entity_id == '']\n",
        "        for t_index, t_row in temp.iterrows():\n",
        "          box = build_bb([t_row.Bottom, t_row.Right, t_row.Top, t_row.Left])\n",
        "          if overlap2(scene_box, box):\n",
        "            df.at[index, 'entity_id'] = entityId\n",
        "            df.at[index, 'product_id'] = product_id\n",
        "            df.at[index, 'fileName'] = file_name\n",
        "      except:\n",
        "        pass"
      ],
      "id": "RHSXALsxDVHp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aDOYaL5UgssL"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/Temp_Geo_data_new.csv')"
      ],
      "id": "aDOYaL5UgssL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPzka4XC9Zcj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Temp_Geo_data_new.csv')"
      ],
      "id": "bPzka4XC9Zcj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tia2JhtLlu4"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# e_ids, p_ids = df[~pd.isna(df.fileName)].iloc[0:10]['entity_id'].tolist(), df[~pd.isna(df.fileName)].iloc[0:10]['product_id'].tolist()\n",
        "# with open('data.json', 'r') as openfile:\n",
        "#   json_file = json.load(openfile)\n",
        "#   username, password = json_file['username'], json_file['password']\n",
        "#   login(username, password)\n",
        "#   r = bulk_download_scene(e_ids, p_ids)\n",
        "#   while len(r['data']['preparingDownloads']) > 0:\n",
        "#     r = bulk_download_scene(e_ids, p_ids)\n",
        "#     print(r)"
      ],
      "id": "4tia2JhtLlu4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPSh-nbtYm1d"
      },
      "outputs": [],
      "source": [
        "def get_bounding_box(x_dim, y_dim, lat_min, lon_min, lat_max, lon_max):\n",
        "  return x_dim*lon_min, x_dim*lon_max, y_dim*lat_min, y_dim*lat_max"
      ],
      "id": "cPSh-nbtYm1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dguT6xYlA5RF"
      },
      "outputs": [],
      "source": [
        "df = df[~pd.isna(df.fileName)]"
      ],
      "id": "dguT6xYlA5RF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y06SgxxtROZX"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ],
      "id": "y06SgxxtROZX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frbi2eLg-rcR"
      },
      "outputs": [],
      "source": [
        "import utm\n",
        "from osgeo import gdal\n",
        "def get_utm(lon, lat, imgfile, x_or_y):\n",
        "  try:\n",
        "    u = utm.from_latlon(float(lat), float(lon))\n",
        "\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "\n",
        "    filename = '/content/data/' + str(imgfile).lower() + '.tif' #path to raster\n",
        "\n",
        "    dataset = gdal.Open(filename)\n",
        "    band = dataset.GetRasterBand(1)\n",
        "\n",
        "    cols = dataset.RasterXSize\n",
        "    rows = dataset.RasterYSize\n",
        "\n",
        "    transform = dataset.GetGeoTransform()\n",
        "\n",
        "    xOrigin = transform[0]\n",
        "    yOrigin = transform[3]\n",
        "    pixelWidth = transform[1]\n",
        "    pixelHeight = -transform[5]\n",
        "\n",
        "    data = band.ReadAsArray(0, 0, cols, rows)\n",
        "\n",
        "    points_list = [(u[0], u[1])] #list of X,Y coordinates\n",
        "\n",
        "    for point in points_list:\n",
        "        col = int((point[0] - xOrigin) / pixelWidth)\n",
        "        row = int((yOrigin - point[1] ) / pixelHeight)\n",
        "    if x_or_y == 'x':\n",
        "      return col\n",
        "    return row\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "# df['TopLeft_x'] = df.apply(lambda x: get_utm(x.Left, x.Bottom, x.fileName, 'x'), axis=1)\n",
        "# df['TopLeft_y'] = df.apply(lambda x: get_utm(x.Left, x.Bottom, x.fileName, 'y'), axis=1)\n",
        "\n",
        "# df['BottomRight_x'] = df.apply(lambda x: get_utm(x.Right, x.Top, x.fileName, 'x'), axis=1)\n",
        "# df['BottomRight_y'] = df.apply(lambda x: get_utm(x.Right, x.Top, x.fileName, 'y'), axis=1)"
      ],
      "id": "Frbi2eLg-rcR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGAvQNEFERgc"
      },
      "outputs": [],
      "source": [
        "df['TopLeft_x'] = 0\n",
        "df['TopLeft_y'] = 0\n",
        "\n",
        "df['BottomRight_x'] = 0\n",
        "df['BottomRight_y'] = 0"
      ],
      "id": "aGAvQNEFERgc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8aQH9azE9Ya"
      },
      "outputs": [],
      "source": [
        "print(r.keys())"
      ],
      "id": "f8aQH9azE9Ya"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg2cRLoBG3lK"
      },
      "outputs": [],
      "source": [
        "# for d in r['data']['availableDownloads']:\n",
        "#     remove = remove_download(d['downloadId'])"
      ],
      "id": "rg2cRLoBG3lK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "618hOCJ3VePl"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "id": "618hOCJ3VePl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV15o9veVika"
      },
      "outputs": [],
      "source": [
        "!rm -r data\n",
        "!rm -r temp"
      ],
      "id": "RV15o9veVika"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzGu2ObRWRed"
      },
      "outputs": [],
      "source": [
        "a = None\n",
        "with open('data.json', 'r') as openfile:\n",
        "    json_file = json.load(openfile)\n",
        "    username, password = json_file['username'], json_file['password']\n",
        "    login(username, password)\n",
        "    a = download_retrieve()"
      ],
      "id": "BzGu2ObRWRed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozQGujgYgqTY"
      },
      "outputs": [],
      "source": [
        "!rm -r data\n",
        "!rm -r temp"
      ],
      "id": "ozQGujgYgqTY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "URp2s4gbCYyx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "last_index = 0\n",
        "# download_request_mapping = {}\n",
        "# with open('data.json', 'r') as openfile:\n",
        "#     json_file = json.load(openfile)\n",
        "#     username, password = json_file['username'], json_file['password']\n",
        "#     login(username, password)\n",
        "#     dr = download_retrieve()\n",
        "#     for d in dr['data']['available']:\n",
        "#       if d['displayId'] not in download_request_mapping.keys():\n",
        "#         download_request_mapping[d['displayId']] = d['url']\n",
        "\n",
        "for i in tqdm(range(10, df.shape[0], 10), position=0, leave=True):\n",
        "  e_ids, p_ids, filenames = df.iloc[last_index:i]['entity_id'].tolist(), df.iloc[last_index:i]['product_id'].tolist(), df.iloc[last_index:i]['fileName'].tolist()\n",
        "  with open('data.json', 'r') as openfile:\n",
        "    json_file = json.load(openfile)\n",
        "    username, password = json_file['username'], json_file['password']\n",
        "    login(username, password)\n",
        "    # for idx, fn in enumerate(filenames):\n",
        "    #   if fn not in download_request_mapping.keys():\n",
        "    #     prev_len = len(download_retrieve()['data']['available'])\n",
        "    #     r = bulk_download_scene([e_ids[idx]], [p_ids[idx]])\n",
        "    #     while prev_len == len(download_retrieve()['data']['available']):\n",
        "    #       pass\n",
        "    #     download_request_mapping[fn] = download_retrieve()['data']['available'][0]['url']\n",
        "\n",
        "    r = bulk_download_scene(e_ids, p_ids)\n",
        "\n",
        "    download_urls = []\n",
        "    for down in r['data']['availableDownloads']:\n",
        "      download_urls.append(down['url'])\n",
        "\n",
        "    for down in r['data']['preparingDownloads']:\n",
        "      download_urls.append(down['url'])\n",
        "\n",
        "\n",
        "    #print('Number downloads = ' + str(len(download_urls)))\n",
        "    !mkdir temp\n",
        "    def download(url):\n",
        "      wget.download(url, '/content/temp')\n",
        "    #print('Downloading images')\n",
        "    res = ThreadPool(20)\n",
        "    res.imap_unordered(download, download_urls)\n",
        "    res.close()\n",
        "    res.join()\n",
        "\n",
        "    #print('Unzipping image files')\n",
        "    !mkdir data\n",
        "    def unzip(filename):\n",
        "      with zipfile.ZipFile(filename, 'r') as f:\n",
        "        f.extractall('./data/')\n",
        "\n",
        "    zip_files = glob('./temp/*.ZIP')\n",
        "    res = ThreadPool(20)\n",
        "    res.imap_unordered(unzip, zip_files)\n",
        "    res.close()\n",
        "    res.join()\n",
        "\n",
        "    #print('Getting UTM pixel coordinates')\n",
        "    #print(df.iloc[last_index:i].head(10))\n",
        "    df.iloc[last_index:i]['TopLeft_x'] = df.iloc[last_index:i].swifter.apply(lambda x: get_utm(x.Left, x.Bottom, x.fileName, 'x'), axis=1)\n",
        "    df.iloc[last_index:i]['TopLeft_y'] = df.iloc[last_index:i].swifter.apply(lambda x: get_utm(x.Left, x.Bottom, x.fileName, 'y'), axis=1)\n",
        "\n",
        "    df.iloc[last_index:i]['BottomRight_x'] = df.iloc[last_index:i].swifter.apply(lambda x: get_utm(x.Right, x.Top, x.fileName, 'x'), axis=1)\n",
        "    df.iloc[last_index:i]['BottomRight_y'] = df.iloc[last_index:i].swifter.apply(lambda x: get_utm(x.Right, x.Top, x.fileName, 'y'), axis=1)\n",
        "\n",
        "    to_be_moved = []\n",
        "    downloaded_files = [os.path.basename(x)[0:-4] for x in glob('/content/bucket_data/*')]\n",
        "    for f in glob('/content/data/*'):\n",
        "      if f.split('/')[-1] not in downloaded_files:\n",
        "        to_be_moved.append(f)\n",
        "\n",
        "    def move_data(filename):\n",
        "      new_path = '/content/bucket_data/' + filename.split('/')[-1]\n",
        "      shutil.copy(filename, new_path)\n",
        "    res = ThreadPool(20)\n",
        "    res.imap_unordered(move_data, to_be_moved)\n",
        "    res.close()\n",
        "    res.join()\n",
        "\n",
        "    !rm -r temp\n",
        "    !rm -r data\n",
        "    last_index = i"
      ],
      "id": "URp2s4gbCYyx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKwkrezKcHse"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./GeoData_largest.csv')\n",
        "df.to_csv('/content/drive/MyDrive/GeoData_largest.csv')"
      ],
      "id": "pKwkrezKcHse"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Di71IC3EQfK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ],
      "id": "_Di71IC3EQfK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI6G2-0oFJn-"
      },
      "outputs": [],
      "source": [
        "from d2l import torch as d2l\n",
        "\n",
        "def bbox_to_rect(bbox, color):\n",
        "    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n",
        "    # Convert the bounding box (upper-left x, upper-left y, lower-right x,\n",
        "    # lower-right y) format to the matplotlib format: ((upper-left x,\n",
        "    # upper-left y), width, height)\n",
        "    return d2l.plt.Rectangle(\n",
        "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n",
        "        fill=False, edgecolor=color, linewidth=2)"
      ],
      "id": "FI6G2-0oFJn-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEW3XfZO-mZ8"
      },
      "outputs": [],
      "source": [
        "def get_bbox(pixel_coords):\n",
        "  return [pixel_coords[2],pixel_coords[3],pixel_coords[0],pixel_coords[1]]\n",
        "\n",
        "df['bbox'] = df[['TopLeft_x', 'TopLeft_y', 'BottomRight_x', 'BottomRight_y']].apply(lambda x:\n",
        "                                                                                    get_bbox([x.TopLeft_x, x.TopLeft_y, x.BottomRight_x, x.BottomRight_y]),axis=1)"
      ],
      "id": "IEW3XfZO-mZ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bplhc7Qz_FJG"
      },
      "outputs": [],
      "source": [
        "def combine_bboxes(bboxes):\n",
        "  result = []\n",
        "  for b in bboxes:\n",
        "    result.append(b)\n",
        "  return result\n",
        "\n",
        "def combine_labels(labels):\n",
        "  result = []\n",
        "  for l in labels:\n",
        "    result.append(l)\n",
        "  return result\n",
        "\n",
        "data_df = df.groupby(['fileName']).agg({'bbox':lambda x: combine_bboxes(x), 'Class':lambda x: combine_labels(x),\n",
        "                                        'entity_id':'min', 'product_id':'min'}).reset_index()"
      ],
      "id": "bplhc7Qz_FJG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXeviCqZXQQK"
      },
      "outputs": [],
      "source": [
        "data_df.to_csv('/content/drive/MyDrive/ATML_Dataset_Largest.csv')"
      ],
      "id": "iXeviCqZXQQK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNGSfzoaTElm"
      },
      "outputs": [],
      "source": [
        "df[df.TopLeft_x == 0].shape"
      ],
      "id": "FNGSfzoaTElm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-rvGXfhWOEY"
      },
      "outputs": [],
      "source": [
        "data_df.head()"
      ],
      "id": "H-rvGXfhWOEY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvZ67ReKXDSe"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "temp = df.iloc[107]\n",
        "\n",
        "path = '/content/bucket_data/' + temp.fileName.lower() + '.tif'\n",
        "print(path)\n",
        "with Image.open(path) as image:\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(image)\n",
        "    print(image.size)\n",
        "    # Create a Rectangle patch\n",
        "\n",
        "    # Add the patch to the Axes\n",
        "    for index, row in df[df.fileName == temp.fileName].iterrows():\n",
        "      ax.add_patch(bbox_to_rect([row.TopLeft_x, row.TopLeft_y, row.BottomRight_x, row.BottomRight_y], 'red'))\n",
        "      ax.annotate(row.Class, ((row.TopLeft_x + row.BottomRight_x)/2.2,\n",
        "                              row.BottomRight_y-100))\n",
        "\n",
        "    plt.show()\n"
      ],
      "id": "EvZ67ReKXDSe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3xjKmu7X6z"
      },
      "source": [
        "Build dataset for pytorch"
      ],
      "id": "JA3xjKmu7X6z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRUrqUnS7a3n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "id": "eRUrqUnS7a3n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx60zOzs7qU_"
      },
      "outputs": [],
      "source": [
        "class VHRSatelliteDataset(Dataset):\n",
        "  def __init__(self, fileNames, boxes, labels, entities, products, transforms=None):\n",
        "    self.files = fileNames\n",
        "    self.boxes = boxes\n",
        "    self.labels = labels\n",
        "    self.entities = entities\n",
        "    self.products = products\n",
        "    self.transforms = transforms\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_path = self.files[idx]\n",
        "    label = self.labels[idx]\n",
        "    boxes = self.boxes[idx]\n",
        "\n",
        "    e_ids, p_ids = [self.entities[idx]], [self.products[idx]]\n",
        "    r = None\n",
        "    with open('data.json', 'r') as openfile:\n",
        "      json_file = json.load(openfile)\n",
        "      username, password = json_file['username'], json_file['password']\n",
        "      login(username, password)\n",
        "      r = bulk_download_scene(e_ids, p_ids)\n",
        "      while len(r['data']['preparingDownloads']) > 0:\n",
        "        r = bulk_download_scene(e_ids, p_ids)\n",
        "\n",
        "    !mkdir temp\n",
        "    def download(url):\n",
        "      wget.download(url['url'], '/content/temp')\n",
        "\n",
        "    resDownload = ThreadPool(20)\n",
        "    resDownload.imap_unordered(download, r['data']['availableDownloads'])\n",
        "    resDownload.close()\n",
        "    resDownload.join()\n",
        "\n",
        "    !mkdir data\n",
        "    def unzip(filename):\n",
        "      with zipfile.ZipFile(filename, 'r') as f:\n",
        "        f.extractall('./data/')\n",
        "\n",
        "    zip_files = glob('./temp/*.ZIP')\n",
        "    res = ThreadPool(20)\n",
        "    res.imap_unordered(unzip, zip_files)\n",
        "    res.close()\n",
        "    res.join()\n",
        "\n",
        "    path = '/content/data/' + file_path.lower() + '.tif'\n",
        "\n",
        "    img = Image.open(path)\n",
        "\n",
        "    return img, label, boxes"
      ],
      "id": "hx60zOzs7qU_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mifOZqCPBK2m"
      },
      "outputs": [],
      "source": [
        "data = VHRSatelliteDataset(data_df['fileName'].tolist(), data_df['bbox'].tolist(), data_df['Class'].tolist(),\n",
        "                           data_df['entity_id'].tolist(),data_df['product_id'].tolist())"
      ],
      "id": "mifOZqCPBK2m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4DyVJReBTKw"
      },
      "outputs": [],
      "source": [
        "a = DataLoader(data, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "id": "f4DyVJReBTKw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgJo0_3GfDgs"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "id": "WgJo0_3GfDgs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htXjRxZdfIl3"
      },
      "outputs": [],
      "source": [
        "!mkdir googleBucketFolder\n",
        "!gcsfuse --implicit-dirs colab-connect-bucket googleBucketFolder"
      ],
      "id": "htXjRxZdfIl3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UI75f0HEFix"
      },
      "outputs": [],
      "source": [
        "!mkdir bucket_data\n",
        "!gcsfuse --implicit-dirs atml_bucket bucket_data"
      ],
      "id": "-UI75f0HEFix"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZoyHyJvCvwT"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for _, data in a:\n",
        "  pass\n",
        "  !rm -r data\n",
        "  !rm -r temp"
      ],
      "id": "ZZoyHyJvCvwT"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}